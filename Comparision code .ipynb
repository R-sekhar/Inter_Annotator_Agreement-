{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c36eddba-5b86-4271-8179-34bb4d72b3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Agreement Metrics:\n",
      "  Overall Agreement (row-level): 0.42\n",
      "  Accuracy: 0.94\n",
      "  Macro F1 Score: 0.73\n",
      "  Cohen's Kappa: 0.92\n",
      "Comparison results have been saved to 'Final_Result.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, cohen_kappa_score\n",
    "\n",
    "# --- Step 1: Read Annotator Data ---\n",
    "# Replace these with your actual Excel file names.\n",
    "df_a1 = pd.read_excel('annotator1.xlsx')\n",
    "df_a2 = pd.read_excel('annotator2.xlsx')\n",
    "\n",
    "# --- Step 1.1: Clean Column Names ---\n",
    "df_a1.columns = df_a1.columns.str.strip()\n",
    "df_a2.columns = df_a2.columns.str.strip()\n",
    "\n",
    "# --- Step 1.2: Rename Columns if Needed ---\n",
    "if 'Argument' not in df_a1.columns and 'Arguments' in df_a1.columns:\n",
    "    df_a1.rename(columns={'Arguments': 'Argument'}, inplace=True)\n",
    "if 'Argument' not in df_a2.columns and 'Arguments' in df_a2.columns:\n",
    "    df_a2.rename(columns={'Arguments': 'Argument'}, inplace=True)\n",
    "\n",
    "# --- Step 1.3: Preserve Original Order from Annotator1 ---\n",
    "df_a1['order'] = df_a1.index\n",
    "\n",
    "# --- Step 2: Select Only Relevant Columns ---\n",
    "df_a1 = df_a1[['Argument', 'Sense', 'order']]\n",
    "df_a2 = df_a2[['Argument', 'Sense']]\n",
    "\n",
    "# Rename Sense columns to distinguish annotators.\n",
    "df_a1.rename(columns={'Sense': 'Sense_a1'}, inplace=True)\n",
    "df_a2.rename(columns={'Sense': 'Sense_a2'}, inplace=True)\n",
    "\n",
    "# --- Step 3: Merge Data on \"Argument\" Field ---\n",
    "merged_df = pd.merge(df_a1, df_a2, on='Argument', how='outer', sort=False)\n",
    "\n",
    "# --- Step 4: Fill Missing \"order\" Values for Rows from Annotator2 Only ---\n",
    "# For rows coming only from annotator2, assign them an order value greater than any in annotator1.\n",
    "max_order = df_a1['order'].max() if not df_a1.empty else 0\n",
    "merged_df['order'] = merged_df['order'].fillna(\n",
    "    pd.Series(max_order + merged_df.index.to_series() + 1, index=merged_df.index)\n",
    ")\n",
    "\n",
    "# --- Step 5: Replace Missing Sense Values with \"N/A\" ---\n",
    "merged_df['Sense_a1'] = merged_df['Sense_a1'].fillna(\"N/A\")\n",
    "merged_df['Sense_a2'] = merged_df['Sense_a2'].fillna(\"N/A\")\n",
    "\n",
    "# --- Step 6: Drop Rows Where Both Annotators Have \"N/A\" ---\n",
    "merged_df = merged_df[~((merged_df['Sense_a1'] == \"N/A\") & (merged_df['Sense_a2'] == \"N/A\"))]\n",
    "\n",
    "# --- Step 7: Compute Row-Level Agreement ---\n",
    "# Compare sense labels in a case-insensitive manner (using lower() and strip()).\n",
    "def compute_agreement(row):\n",
    "    s1 = row['Sense_a1'].lower().strip()\n",
    "    s2 = row['Sense_a2'].lower().strip()\n",
    "    if s1 == \"n/a\" or s2 == \"n/a\":\n",
    "        return 0\n",
    "    return 1 if s1 == s2 else 0\n",
    "\n",
    "merged_df['Agreement'] = merged_df.apply(compute_agreement, axis=1)\n",
    "\n",
    "# --- Step 8: Sort by Original Order ---\n",
    "merged_df = merged_df.sort_values(by='order').drop(columns=['order'])\n",
    "\n",
    "# --- Step 9: Compute Global Metrics ---\n",
    "# Consider only rows where both annotators provided a sense (i.e. not \"N/A\").\n",
    "valid_df = merged_df[(merged_df['Sense_a1'] != \"N/A\") & (merged_df['Sense_a2'] != \"N/A\")]\n",
    "y_true = valid_df['Sense_a1'].str.lower().str.strip()\n",
    "y_pred = valid_df['Sense_a2'].str.lower().str.strip()\n",
    "\n",
    "overall_agreement = merged_df['Agreement'].mean()\n",
    "accuracy = accuracy_score(y_true, y_pred) if not valid_df.empty else 0\n",
    "f1 = f1_score(y_true, y_pred, average='macro') if not valid_df.empty else 0\n",
    "kappa = cohen_kappa_score(y_true, y_pred) if not valid_df.empty else 0\n",
    "\n",
    "print(\"Global Agreement Metrics:\")\n",
    "print(f\"  Overall Agreement (row-level): {overall_agreement:.2f}\")\n",
    "print(f\"  Accuracy: {accuracy:.2f}\")\n",
    "print(f\"  Macro F1 Score: {f1:.2f}\")\n",
    "print(f\"  Cohen's Kappa: {kappa:.2f}\")\n",
    "\n",
    "# --- Step 10: Save the Comparison Results to an Excel File ---\n",
    "output_filename = \"Final_Result.xlsx\"\n",
    "merged_df.to_excel(output_filename, index=False)\n",
    "print(f\"Comparison results have been saved to '{output_filename}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35bf728a-37e5-4981-ab8c-e19225cbfd64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Agreement Metrics:\n",
      "  Overall Agreement (row-level): 0.41\n",
      "  Accuracy: 0.94\n",
      "  Macro F1 Score: 0.73\n",
      "  Cohen's Kappa: 0.93\n",
      "Comparison results have been saved to 'Retest.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, cohen_kappa_score\n",
    "\n",
    "# --- Step 1: Read Annotator Data ---\n",
    "# Replace these file names with your actual Excel file names.\n",
    "df_a1 = pd.read_excel('annotator1.xlsx')\n",
    "df_a2 = pd.read_excel('annotator2.xlsx')\n",
    "\n",
    "# --- Step 1.1: Clean Column Names ---\n",
    "df_a1.columns = df_a1.columns.str.strip()\n",
    "df_a2.columns = df_a2.columns.str.strip()\n",
    "\n",
    "# --- Step 1.2: Rename Columns if Needed ---\n",
    "# For annotator 1:\n",
    "if 'Argument' not in df_a1.columns and 'Arguments' in df_a1.columns:\n",
    "    df_a1.rename(columns={'Arguments': 'Argument'}, inplace=True)\n",
    "if 'Argument2' not in df_a1.columns and 'Arguments2' in df_a1.columns:\n",
    "    df_a1.rename(columns={'Arguments2': 'Argument2'}, inplace=True)\n",
    "if 'Sense' not in df_a1.columns and 'Senses' in df_a1.columns:\n",
    "    df_a1.rename(columns={'Senses': 'Sense'}, inplace=True)\n",
    "if 'Sense2' not in df_a1.columns and 'Senses2' in df_a1.columns:\n",
    "    df_a1.rename(columns={'Senses2': 'Sense2'}, inplace=True)\n",
    "\n",
    "# For annotator 2:\n",
    "if 'Argument' not in df_a2.columns and 'Arguments' in df_a2.columns:\n",
    "    df_a2.rename(columns={'Arguments': 'Argument'}, inplace=True)\n",
    "if 'Argument2' not in df_a2.columns and 'Arguments2' in df_a2.columns:\n",
    "    df_a2.rename(columns={'Arguments2': 'Argument2'}, inplace=True)\n",
    "if 'Sense' not in df_a2.columns and 'Senses' in df_a2.columns:\n",
    "    df_a2.rename(columns={'Senses': 'Sense'}, inplace=True)\n",
    "if 'Sense2' not in df_a2.columns and 'Senses2' in df_a2.columns:\n",
    "    df_a2.rename(columns={'Senses2': 'Sense2'}, inplace=True)\n",
    "\n",
    "# --- Step 1.3: (Optional) Preserve Order from Annotator1 ---\n",
    "df_a1['order'] = df_a1.index\n",
    "\n",
    "# --- Step 2: Reshape Each Annotator's Data to Long Format ---\n",
    "# Concatenate the two sets of columns (first and second relations) for each annotator.\n",
    "df_a1_long = pd.DataFrame({\n",
    "    \"Argument\": pd.concat([df_a1[\"Argument\"], df_a1[\"Argument2\"]], ignore_index=True),\n",
    "    \"Sense_a1\": pd.concat([df_a1[\"Sense\"], df_a1[\"Sense2\"]], ignore_index=True)\n",
    "})\n",
    "df_a2_long = pd.DataFrame({\n",
    "    \"Argument\": pd.concat([df_a2[\"Argument\"], df_a2[\"Argument2\"]], ignore_index=True),\n",
    "    \"Sense_a2\": pd.concat([df_a2[\"Sense\"], df_a2[\"Sense2\"]], ignore_index=True)\n",
    "})\n",
    "\n",
    "# --- Step 3: Merge the Two Long DataFrames on \"Argument\" ---\n",
    "merged_df = pd.merge(df_a1_long, df_a2_long, on=\"Argument\", how=\"outer\", sort=False)\n",
    "\n",
    "# --- Step 4: Process Missing Values ---\n",
    "# Fill missing sense values with \"N/A\"\n",
    "merged_df[\"Sense_a1\"] = merged_df[\"Sense_a1\"].fillna(\"N/A\")\n",
    "merged_df[\"Sense_a2\"] = merged_df[\"Sense_a2\"].fillna(\"N/A\")\n",
    "\n",
    "# Drop rows where both annotators have no annotation (i.e. both are \"N/A\")\n",
    "merged_df = merged_df[~((merged_df[\"Sense_a1\"] == \"N/A\") & (merged_df[\"Sense_a2\"] == \"N/A\"))]\n",
    "\n",
    "# --- Step 5: Compute Row-Level Agreement ---\n",
    "# Compare sense labels in a case-insensitive manner.\n",
    "def compute_agreement(row):\n",
    "    s1 = row[\"Sense_a1\"].lower().strip()\n",
    "    s2 = row[\"Sense_a2\"].lower().strip()\n",
    "    if s1 == \"n/a\" or s2 == \"n/a\":\n",
    "        return 0\n",
    "    return 1 if s1 == s2 else 0\n",
    "\n",
    "merged_df[\"Agreement\"] = merged_df.apply(compute_agreement, axis=1)\n",
    "\n",
    "# --- Step 6: Compute Global Metrics ---\n",
    "# Use only rows where both annotators provided a sense (i.e. not \"N/A\")\n",
    "valid_df = merged_df[(merged_df[\"Sense_a1\"] != \"N/A\") & (merged_df[\"Sense_a2\"] != \"N/A\")]\n",
    "y_true = valid_df[\"Sense_a1\"].str.lower().str.strip()\n",
    "y_pred = valid_df[\"Sense_a2\"].str.lower().str.strip()\n",
    "\n",
    "overall_agreement = merged_df[\"Agreement\"].mean()\n",
    "accuracy = accuracy_score(y_true, y_pred) if not valid_df.empty else 0\n",
    "f1 = f1_score(y_true, y_pred, average=\"macro\") if not valid_df.empty else 0\n",
    "kappa = cohen_kappa_score(y_true, y_pred) if not valid_df.empty else 0\n",
    "\n",
    "print(\"Global Agreement Metrics:\")\n",
    "print(f\"  Overall Agreement (row-level): {overall_agreement:.2f}\")\n",
    "print(f\"  Accuracy: {accuracy:.2f}\")\n",
    "print(f\"  Macro F1 Score: {f1:.2f}\")\n",
    "print(f\"  Cohen's Kappa: {kappa:.2f}\")\n",
    "\n",
    "# --- Step 7: Save the Final Merged Comparison to an Excel File ---\n",
    "output_filename = \"Retest.xlsx\"\n",
    "merged_df.to_excel(output_filename, index=False)\n",
    "print(f\"Comparison results have been saved to '{output_filename}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "47e7c0bc-ff86-407b-83d6-4ffde35923f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Agreement Metrics:\n",
      "  Overall Agreement (row-level): 0.62\n",
      "  Accuracy: 0.98\n",
      "  Macro F1 Score: 0.79\n",
      "  Cohen's Kappa: 0.97\n",
      "Comparison results have been saved to 'water_mainbreak_agreement.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, cohen_kappa_score\n",
    "\n",
    "# --- Step 1: Read Annotator Data ---\n",
    "# Replace these file names with your actual Excel file names.\n",
    "df_a1 = pd.read_excel('Annotator1_water_mainbreak.xlsx')\n",
    "df_a2 = pd.read_excel('Annotator2_water_mainbreak.xlsx')\n",
    "\n",
    "# --- Step 1.1: Clean Column Names ---\n",
    "df_a1.columns = df_a1.columns.str.strip()\n",
    "df_a2.columns = df_a2.columns.str.strip()\n",
    "\n",
    "# --- Step 1.2: Rename Columns if Needed ---\n",
    "# For annotator 1:\n",
    "if 'Argument' not in df_a1.columns and 'Arguments' in df_a1.columns:\n",
    "    df_a1.rename(columns={'Arguments': 'Argument'}, inplace=True)\n",
    "if 'Argument2' not in df_a1.columns and 'Arguments2' in df_a1.columns:\n",
    "    df_a1.rename(columns={'Arguments2': 'Argument2'}, inplace=True)\n",
    "if 'Sense' not in df_a1.columns and 'Senses' in df_a1.columns:\n",
    "    df_a1.rename(columns={'Senses': 'Sense'}, inplace=True)\n",
    "if 'Sense2' not in df_a1.columns and 'Senses2' in df_a1.columns:\n",
    "    df_a1.rename(columns={'Senses2': 'Sense2'}, inplace=True)\n",
    "\n",
    "# For annotator 2:\n",
    "if 'Argument' not in df_a2.columns and 'Arguments' in df_a2.columns:\n",
    "    df_a2.rename(columns={'Arguments': 'Argument'}, inplace=True)\n",
    "if 'Argument2' not in df_a2.columns and 'Arguments2' in df_a2.columns:\n",
    "    df_a2.rename(columns={'Arguments2': 'Argument2'}, inplace=True)\n",
    "if 'Sense' not in df_a2.columns and 'Senses' in df_a2.columns:\n",
    "    df_a2.rename(columns={'Senses': 'Sense'}, inplace=True)\n",
    "if 'Sense2' not in df_a2.columns and 'Senses2' in df_a2.columns:\n",
    "    df_a2.rename(columns={'Senses2': 'Sense2'}, inplace=True)\n",
    "\n",
    "# --- Step 1.3: (Optional) Preserve Order from Annotator1 ---\n",
    "df_a1['order'] = df_a1.index\n",
    "\n",
    "# --- Step 2: Reshape Each Annotator's Data to Long Format ---\n",
    "# Concatenate the two sets of columns (first and second relations) for each annotator.\n",
    "df_a1_long = pd.DataFrame({\n",
    "    \"Argument\": pd.concat([df_a1[\"Argument\"], df_a1[\"Argument2\"]], ignore_index=True),\n",
    "    \"Sense_a1\": pd.concat([df_a1[\"Sense\"], df_a1[\"Sense2\"]], ignore_index=True)\n",
    "})\n",
    "df_a2_long = pd.DataFrame({\n",
    "    \"Argument\": pd.concat([df_a2[\"Argument\"], df_a2[\"Argument2\"]], ignore_index=True),\n",
    "    \"Sense_a2\": pd.concat([df_a2[\"Sense\"], df_a2[\"Sense2\"]], ignore_index=True)\n",
    "})\n",
    "\n",
    "# --- Step 3: Merge the Two Long DataFrames on \"Argument\" ---\n",
    "merged_df = pd.merge(df_a1_long, df_a2_long, on=\"Argument\", how=\"outer\", sort=False)\n",
    "\n",
    "# --- Step 4: Process Missing Values ---\n",
    "# Fill missing sense values with \"N/A\"\n",
    "merged_df[\"Sense_a1\"] = merged_df[\"Sense_a1\"].fillna(\"N/A\")\n",
    "merged_df[\"Sense_a2\"] = merged_df[\"Sense_a2\"].fillna(\"N/A\")\n",
    "\n",
    "# Drop rows where both annotators have no annotation (i.e. both are \"N/A\")\n",
    "merged_df = merged_df[~((merged_df[\"Sense_a1\"] == \"N/A\") & (merged_df[\"Sense_a2\"] == \"N/A\"))]\n",
    "\n",
    "# --- Step 5: Compute Row-Level Agreement ---\n",
    "# Compare sense labels in a case-insensitive manner.\n",
    "def compute_agreement(row):\n",
    "    s1 = row[\"Sense_a1\"].lower().strip()\n",
    "    s2 = row[\"Sense_a2\"].lower().strip()\n",
    "    if s1 == \"n/a\" or s2 == \"n/a\":\n",
    "        return 0\n",
    "    return 1 if s1 == s2 else 0\n",
    "\n",
    "merged_df[\"Agreement\"] = merged_df.apply(compute_agreement, axis=1)\n",
    "\n",
    "# --- Step 6: Compute Global Metrics ---\n",
    "# Use only rows where both annotators provided a sense (i.e. not \"N/A\")\n",
    "valid_df = merged_df[(merged_df[\"Sense_a1\"] != \"N/A\") & (merged_df[\"Sense_a2\"] != \"N/A\")]\n",
    "y_true = valid_df[\"Sense_a1\"].str.lower().str.strip()\n",
    "y_pred = valid_df[\"Sense_a2\"].str.lower().str.strip()\n",
    "\n",
    "overall_agreement = merged_df[\"Agreement\"].mean()\n",
    "accuracy = accuracy_score(y_true, y_pred) if not valid_df.empty else 0\n",
    "f1 = f1_score(y_true, y_pred, average=\"macro\") if not valid_df.empty else 0\n",
    "kappa = cohen_kappa_score(y_true, y_pred) if not valid_df.empty else 0\n",
    "\n",
    "print(\"Global Agreement Metrics:\")\n",
    "print(f\"  Overall Agreement (row-level): {overall_agreement:.2f}\")\n",
    "print(f\"  Accuracy: {accuracy:.2f}\")\n",
    "print(f\"  Macro F1 Score: {f1:.2f}\")\n",
    "print(f\"  Cohen's Kappa: {kappa:.2f}\")\n",
    "\n",
    "# --- Step 7: Save the Final Merged Comparison to an Excel File ---\n",
    "output_filename = \"water_mainbreak_agreement.xlsx\"\n",
    "merged_df.to_excel(output_filename, index=False)\n",
    "print(f\"Comparison results have been saved to '{output_filename}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4060a88e-0330-4ab4-a887-a9144f1a6cec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d94d73-8ae1-471b-a11f-ddc243cba310",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
